Lucas Dale (ld17852), Igor Dolecki (id17002), Todor Hristov (th17418), Theano Xirouchaki (tx17079)

Evaluation
==========
We evaluated our product with a number of relevant stakeholders on two separate occasions: once before the beta release, and once before the final release. In the first case, we held a big meeting with some teachers, the director of the summer program, the general director and deputy director of CELFS and people responsible for admin and IT. We demonstrated the functionality of our website and received comments and constructive criticism from the attendees. We acted upon their feedback by making changes they recommended, including throwing an error if student ID is not in the database, allowing admins to export the marks as an excel file, hiding the overall mark in the review page and considering accessibility issues that might arise from the size of the text or the color scheme.

In the second case, our client asked some of her colleagues to spend thirty minutes to an hour with one of us, marking a Micro Research Report with the old system, marking another report of the same length and type with the new system and then filling in a short Google forms survey asking, in the form of a Likert Scale, how intuitive our system is, how visually pleasing the website is and, in the form of open ended questions, which system they would prefer to use and why, what they liked about our system and what they would change about it. For each participant we randomly selected which essay they should mark first using which system as our client expressed concerns that having already marked one essay would reduce the time needed for the second one.

We got eight teachers to participate and we found that, on average, our system took 5.7% longer than the current system. However, this does not account for the time the current system requires to scan files and put marks in the database. After recording our client input marks in the database, we found that it takes about two minutes per essay. Adding that to the time it took to do the marking with the old system and recalculating, we found that our system is actually 7.5% faster. Although that is not massive, we are quite happy with it as it was the first time using the system and we are confident that the time needed would decrease with practice. We also observed the users and noted which parts of the website they struggled with to ensure they are explained sufficiently in the user manual, which we believe would also help.

Moreover, six of the eight teachers said they would rather use our system, one was on the fence, and only one said they would prefer the old system. Moreover, all of the participants agreed the website was intuitive and all but one agreed that it was visually pleasant. In fact, a participant commented that it is very similar to an alternative version of the old system that she created for herself as she had a hard time using the standardised marking sheet.

We also noted, based on the results to the survey's open questions, that, aside from some mistakes in the text displayed, which we fixed, there was little we could change without making some party unhappy. For example, in the big meeting it was agreed that the page for marking the essays should not display numerical marks to avoid bias, but some of the teachers in the second stage of the evaluation mentioned they would prefer to be able to see the numerical mark before submitting.

All in all, we are believe our final product satisfies both most of the needs and most of the preferences of our client.
